# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CCOHEmcLgINZQsAXDOGNpUnf-za-6jH1
"""

import requests
from bs4 import BeautifulSoup

# Step 1: Fetch the HTML content of a webpage
url = "https://en.wikipedia.org/wiki/Virat_Kohli"
response = requests.get(url)
html_content = response.text

# Step 2: Parse the HTML using BeautifulSoup
soup = BeautifulSoup(html_content, 'html.parser')

# Step 3: Extract text content (e.g., from paragraphs <p>)
text = " ".join([p.text for p in soup.find_all('p')])

print(text[:5000])  # Display the first 500 characters of the extracted text

import re

# Tokenize the text into words
tokens = re.findall(r'\b\w+\b', text)
filtered_tokens = [token for token in tokens if token.lower() not in stop_words]
print(filtered_tokens)  # Display the  filtered tokens

print(tokens[:])  # Display the  tokens

# Filter tokens based on `isalpha` and `isdigit`
token_analysis = [(token, token.isalpha(), token.isdigit()) for token in tokens]
token_analysis_filtered = [(token, token.isalpha(), token.isdigit()) for token in filtered_tokens]

# Display a sample of analyzed tokens
for token, is_alpha, is_digit in token_analysis[:10]:
    print(f"Token: {token}, is_alpha: {is_alpha}, is_digit: {is_digit}")

print()
# Display a sample of filtered tokens
for token, is_alpha, is_digit in token_analysis_filtered[:10]:
    print(f"Token: {token}, is_alpha: {is_alpha}, is_digit: {is_digit}")

import pandas as pd

# Create a DataFrame for tokens and their attributes
df = pd.DataFrame(token_analysis, columns=['Token', 'IsAlpha', 'IsDigit'])
df1 = pd.DataFrame(token_analysis_filtered, columns=['Token', 'IsAlpha', 'IsDigit'])

# Display the DataFrame
print(df.head())
print()
print(df1.head())

from scipy.sparse import csr_matrix

# Convert boolean columns into a sparse matrix
sparse_matrix = csr_matrix(df[['IsAlpha', 'IsDigit']].values)

print(sparse_matrix)

pip install nltk

import nltk
from nltk.corpus import stopwords

# Download the stopwords list
nltk.download('stopwords')

# Get the stop words for English
stop_words = set(stopwords.words('english'))

# Tokenize the text into words
tokens = re.findall(r'\b\w+\b', text)

# Filter out stop words
filtered_tokens = [token for token in tokens if token.lower() not in stop_words]

print(filtered_tokens[:10])  # Display the first 10 filtered tokens

